{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3ef2476c-244b-4953-b5df-afb126cfd148",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import PreTrainedTokenizerFast\n",
    "from bertviz.neuron_view import show\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader,Dataset\n",
    "\n",
    "from math import sqrt\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import structure as st\n",
    "\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import WordLevel\n",
    "from tokenizers.trainers import WordLevelTrainer\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "from tokenizers.normalizers import Lowercase\n",
    "from tokenizers.processors import TemplateProcessing\n",
    "from datasets import Dataset as HFdataset\n",
    "\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95b28b1b-1b89-45f2-82ed-5c5f2c9aaf30",
   "metadata": {},
   "source": [
    "<h2>Tokenizing process</h2>\n",
    "\n",
    "There is no need to rerun the cell below,the pretrained token files have been already prepared."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7203a982-e270-482d-bb23-29e07a1e413a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the tokenizer\n",
    "tokenizer = Tokenizer(WordLevel(unk_token=\"[UNK]\"))\n",
    "# normalize sequences to lowercase\n",
    "normalizer = Lowercase\n",
    "tokenizer.normalizer=normalizer() \n",
    "#split sequences only in whitespaces\n",
    "tokenizer.pre_tokenizer = Whitespace()\n",
    "\n",
    "# Define special tokens\n",
    "special_tokens_dict = {'pad_token': '[PAD]','unk_token': '[UNK]','sos_token': '[SOS]', 'eos_token': '[EOS]', 'mask_token': '[MASK]'}\n",
    "\n",
    "# Define the trainer with special tokens\n",
    "trainer = WordLevelTrainer(special_tokens=list(special_tokens_dict.values()))\n",
    "\n",
    "# Example training data (the word sources are s.DNA_Codons.keys() , s.RNA_Codons.keys() and s.tobacco_biased.keys())\n",
    "training_data = st.RNA_Codons.keys()\n",
    "\n",
    "# Save the training data to a temporary file\n",
    "with open(\"training_data.txt\", \"w\") as f:\n",
    "    f.write(\"\\n\".join(training_data))\n",
    "tokenizer.enable_padding()\n",
    "#only for the first and the last poitions of DNAs and RNAs that will be utilized in our decoder section, two special tokens sould be added \n",
    "# tokenizer.post_processor = TemplateProcessing(\n",
    "#             single=\"[SOS] $A [EOS]\",\n",
    "#             special_tokens=[(\"[SOS]\", 2), (\"[EOS]\", 3)],)\n",
    "# Train the tokenizer\n",
    "tokenizer.train([\"training_data.txt\"], trainer)\n",
    "# Save the tokenizer to a file\n",
    "tokenizer.save(\"C:\\\\Users\\\\farsh\\\\Downloads\\\\tokenizer_RNA.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e08710e1-9f4f-485b-a5fa-118a21c06e79",
   "metadata": {},
   "source": [
    "<h2>Directories</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2877bff5-9605-4730-909f-0c88d19316db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #your raw data path\n",
    "# path_amino=\"C:\\\\Users\\\\farsh\\\\OneDrive\\\\Desktop\\\\CodonOptimization\\\\sequence.fasta\"\n",
    "# path_dna=\"C:\\\\Users\\\\farsh\\\\OneDrive\\\\Desktop\\\\CodonOptimization\\\\sequence.txt\"\n",
    "# #pre_trained tokenized file location\n",
    "# tokenizer_DNAfile='C:\\\\Users\\\\farsh\\\\OneDrive\\\\Desktop\\\\CodonOptimization\\\\tokenizer_DNA.json' \n",
    "# tokenizer_RNAfile='C:\\\\Users\\\\farsh\\\\OneDrive\\\\Desktop\\\\CodonOptimization\\\\tokenizer_RNA.json' \n",
    "# tokenizer_AAfile='C:\\\\Users\\\\farsh\\\\OneDrive\\\\Desktop\\\\CodonOptimization\\\\tokenizer_aa.json'\n",
    "# #the best model parameters location to save\n",
    "# checkpoint_dir = \"C:\\\\Users\\\\farsh\\\\OneDrive\\\\Desktop\\\\CodonOptimization\\\\checkpoints\"\n",
    "# #the inference file to validate our algorithm\n",
    "# path_infer=\"C:\\\\Users\\\\farsh\\\\OneDrive\\\\Desktop\\\\CodonOptimization\\\\infer.txt\"\n",
    "# path_save_prediction=\"C:\\\\Users\\\\farsh\\\\OneDrive\\\\Desktop\\\\CodonOptimization\\\\PREDICTED_seq.fasta\"\n",
    "\n",
    "\n",
    "# Define a base directory (e.g. project root or a 'data' folder)\n",
    "base_dir = Path().resolve() # current script's directory\n",
    "\n",
    "# Use relative paths from base_dir\n",
    "path_amino = str(base_dir / \"data\" / \"sequence.fasta\")\n",
    "path_dna = str(base_dir / \"data\" / \"sequence.txt\")\n",
    "\n",
    "tokenizer_DNAfile = str(base_dir / \"tokenizers\" / \"tokenizer_DNA.json\")\n",
    "tokenizer_RNAfile = str(base_dir / \"tokenizers\" / \"tokenizer_RNA.json\")\n",
    "tokenizer_AAfile = str(base_dir / \"tokenizers\" / \"tokenizer_aa.json\")\n",
    "\n",
    "checkpoint_dir = str(base_dir / \"checkpoints\")\n",
    "path_infer = str(base_dir / \"data\" / \"infer.txt\")\n",
    "path_save_prediction = str(base_dir / \"output\" / \"PREDICTED_seq.fasta\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbfd7741-efa7-48a4-883a-ff3f982c6b01",
   "metadata": {},
   "source": [
    "<h2>building your own dataset and making it ready for Transformer.</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1973648a-59d5-4feb-8974-172b7cceec86",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer_DNAfile, tokenizer_RNAfile, tokenizer_AAfile,input_length,output_length):\n",
    "        super().__init__()\n",
    "        self.data = data\n",
    "        self.input_length=input_length\n",
    "        self.output_length=output_length\n",
    "        self.tokenizer_DNAfile = tokenizer_DNAfile\n",
    "        self.tokenizer_RNAfile = tokenizer_RNAfile\n",
    "        self.tokenizer_AAfile = tokenizer_AAfile\n",
    "        self.tokenized_dataset = self._preprocess_dataset()\n",
    "\n",
    "    def _tokenize_data(self, data, data_level):\n",
    "        if data_level == 'DNA':\n",
    "            fast_tokenizer = PreTrainedTokenizerFast(tokenizer_file=self.tokenizer_DNAfile)\n",
    "            length=self.output_length\n",
    "        elif data_level == 'RNA':\n",
    "            fast_tokenizer = PreTrainedTokenizerFast(tokenizer_file=self.tokenizer_RNAfile)\n",
    "            length=self.output_length\n",
    "        else:\n",
    "            fast_tokenizer = PreTrainedTokenizerFast(tokenizer_file=self.tokenizer_AAfile)\n",
    "            length=self.input_length\n",
    "        return fast_tokenizer.batch_encode_plus(data, padding='max_length', return_tensors=\"pt\",max_length=length)\n",
    "\n",
    "    def _preprocess(self, data):\n",
    "        model_input = self._tokenize_data(data['input'], data_level='aa')\n",
    "        labels = self._tokenize_data(data['target'], data_level='DNA')\n",
    "        model_input['labels'] = labels['input_ids']\n",
    "        return model_input\n",
    "\n",
    "    def _preprocess_dataset(self):\n",
    "        tokenized_dataset = self.data.map(self._preprocess, batched=True, remove_columns=['input', 'target'])\n",
    "        return tokenized_dataset\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.tokenized_dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        inputs = self.tokenized_dataset['input_ids'][idx]\n",
    "        labels = self.tokenized_dataset['labels'][idx]\n",
    "        return torch.tensor(inputs), torch.tensor(labels)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42ed9f27-8a26-4147-ad42-564ff756f1b4",
   "metadata": {},
   "source": [
    "<h2>Subclasses for Encoder and Decoder Layers</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fe9f3121-a95c-441d-86ba-43e14563a9e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEmbedding(nn.Module):\n",
    "    def __init__(self,max_position_embeddings,embed_size,dtype=torch.float32,**kwargs):\n",
    "        super().__init__()\n",
    "        assert embed_size%2==0, \"embed size must be even!\"\n",
    "        p,i=np.meshgrid(np.arange(max_position_embeddings), 2*np.arange(embed_size//2))\n",
    "        pos_embed=np.empty((1,max_position_embeddings,embed_size))\n",
    "        pos_embed[0,:,::2]=np.sin(p/10_000**(i/embed_size)).T\n",
    "        pos_embed[0,:,1::2]=np.cos(p/10_000**(i/embed_size)).T\n",
    "        self.pos_encodings = torch.tensor(pos_embed, dtype=dtype)\n",
    "    def forward(self,inputs_ids):\n",
    "        max_length=inputs_ids.size(1)\n",
    "        return inputs_ids+self.pos_encodings[:,:max_length]       \n",
    "\n",
    "class Embeddings(nn.Module):\n",
    "    \"\"\"category can be aa (that stands for amino acid),DNA or RNA\"\"\"\n",
    "    def __init__(self,max_position_embeddings,embed_size,vocab_size,dropout,category=\"aa\"):\n",
    "        super().__init__()\n",
    "        self.category=category\n",
    "        self.vocab_size=vocab_size\n",
    "        self.embed_size=embed_size\n",
    "        self.max_position_embeddings=max_position_embeddings\n",
    "        self.token_embeddings=nn.Embedding(self.vocab_size,self.embed_size,padding_idx=0)\n",
    "        self.position_embeddings=PositionalEmbedding(self.max_position_embeddings,self.embed_size)\n",
    "        self.layer_norm = nn.LayerNorm(self.embed_size, eps=1e-12)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    def forward(self,input_ids):\n",
    "        token_embeddings = self.token_embeddings(input_ids)\n",
    "        position_embeddings = self.position_embeddings(token_embeddings)\n",
    "        embeddings = self.layer_norm(position_embeddings)\n",
    "        embeddings = self.dropout(embeddings)\n",
    "        return embeddings\n",
    "\n",
    "def scaled_dot_product_attention(query, key, value,mask=None):\n",
    "    dim_k=key.size(-1)\n",
    "    \"\"\" for large values of dk (dk is the size of q and k), the dot products grow large in magnitude, pushing the softmax function \n",
    "    into regions where it has extremely small gradients. To counteract this effect, we scale the dot products by sqrt(dim_k)\n",
    "\"\"\"\n",
    "    scores=torch.bmm(query,key.transpose(1,2))/sqrt(dim_k) #This scale (sqrt(dim_k)) was used in the \"Attention is All You Need\" paper\n",
    "    if mask is not None:\n",
    "        mask_matrice=torch.tril(torch.ones(key.size(1),key.size(1))).unsqueeze(0)\n",
    "        scores=scores.masked_fill(mask_matrice==0,-float(\"inf\"))\n",
    "    weights=torch.softmax(scores,dim=-1)\n",
    "    att_output=torch.bmm(weights,value)\n",
    "    return att_output \n",
    "\n",
    "class AttentionHead(nn.Module):\n",
    "    def __init__(self, embed_dim, head_dim):\n",
    "        super().__init__()\n",
    "        self.q = nn.Linear(embed_dim, head_dim)\n",
    "        self.k = nn.Linear(embed_dim, head_dim)\n",
    "        self.v = nn.Linear(embed_dim, head_dim)\n",
    "    def forward(self,hidden_state,decoder_state,mask):\n",
    "        if decoder_state==None:\n",
    "            #To calculate self_attention for encoder and also masked self_attention for decoder layer (by passing mask=True).\n",
    "            attn_outputs = scaled_dot_product_attention(self.q(hidden_state), self.k(hidden_state), self.v(hidden_state),mask=mask)\n",
    "        else:\n",
    "            #To calculate encoder-decoder attention in decoder layer\n",
    "            attn_outputs = scaled_dot_product_attention(self.q(decoder_state), self.k(hidden_state), self.v(hidden_state),mask=mask)\n",
    "        return attn_outputs\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, embed_size,num_attention_heads):\n",
    "        super().__init__()\n",
    "        embed_dim = embed_size\n",
    "        num_heads = num_attention_heads\n",
    "        head_dim = embed_dim // num_heads\n",
    "        self.heads = nn.ModuleList([AttentionHead(embed_dim, head_dim) for _ in range(num_heads)])\n",
    "    def forward(self, hidden_state,decoder_state,mask):\n",
    "        x = torch.cat([h(hidden_state,decoder_state,mask) for h in self.heads], dim=-1)\n",
    "        return x\n",
    "\n",
    "class AddNorm(nn.Module): \n",
    "    \"\"\"The residual connection followed by a layer normalization.\"\"\"\n",
    "    def __init__(self, dropout,embed_size):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.ln = nn.LayerNorm(embed_size)\n",
    "\n",
    "    def forward(self, X, Y):\n",
    "        return self.ln(X+self.dropout(Y))\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, embed_size,intermediate_size):\n",
    "        super().__init__()\n",
    "        self.linear_1 = nn.Linear(embed_size, intermediate_size)\n",
    "        self.linear_2 = nn.Linear(intermediate_size, embed_size)\n",
    "        self.gelu = nn.GELU()\n",
    "    def forward(self, x):\n",
    "        x = self.linear_1(x)\n",
    "        x = self.gelu(x)\n",
    "        x = self.linear_2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ce0210d-c4ed-4a47-8883-0e69fe5dffa3",
   "metadata": {},
   "source": [
    "<h2>Encoder and Decoder Layers</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b6f36124-bc39-427e-b644-746814335840",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoderLayer(nn.Module):\n",
    "    def __init__(self, embed_size,num_attention_heads,intermediate_size,dropout):\n",
    "        super().__init__()\n",
    "        self.attention = MultiHeadAttention(embed_size,num_attention_heads)\n",
    "        self.feed_forward = FeedForward(embed_size,intermediate_size)\n",
    "        self.addnorm = AddNorm(dropout,embed_size) \n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        x1 = self.attention(inputs,decoder_state=None,mask=None)\n",
    "        x2=self.addnorm(inputs,x1)\n",
    "        x3 =self.feed_forward(x2)\n",
    "        x4=self.addnorm(x2,x3)\n",
    "        return x4\n",
    "\n",
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self,num_layers,embed_size,num_attention_heads,intermediate_size,dropout):\n",
    "        super().__init__()\n",
    "        self.num_layer=num_layers\n",
    "        self.encoderLayers=nn.ModuleList([TransformerEncoderLayer(\n",
    "            embed_size,num_attention_heads,intermediate_size,dropout) for _ in range(self.num_layer)])\n",
    "    def forward(self,src):\n",
    "        output=src\n",
    "        for layer in self.encoderLayers:\n",
    "            output=layer(output)\n",
    "        return output\n",
    "  \n",
    "class TransformerDecoderLayer(nn.Module):\n",
    "    def __init__(self,embed_size,num_attention_heads,intermediate_size,dropout):\n",
    "        super().__init__()\n",
    "        self.attention = MultiHeadAttention(embed_size,num_attention_heads)\n",
    "        self.feed_forward = FeedForward(embed_size,intermediate_size)\n",
    "        self.addnorm = AddNorm(dropout,embed_size)\n",
    "        self.encoder_decoder_attention = MultiHeadAttention(embed_size,num_attention_heads)\n",
    "    def forward(self,encoder_output,outputs):\n",
    "        y2 =self.attention(hidden_state=outputs,decoder_state=None,mask=True)\n",
    "        y3=self.addnorm(outputs,y2)\n",
    "        y4 =self.encoder_decoder_attention(hidden_state=encoder_output,decoder_state=y3,mask=None)\n",
    "        y5=self.addnorm(y3,y4)\n",
    "        y6 =self.feed_forward(y5)\n",
    "        y7=self.addnorm(y5,y6)\n",
    "        return y7\n",
    "\n",
    "class TransformerDecoder(nn.Module):\n",
    "    def __init__(self,num_layers,embed_size,num_attention_heads,intermediate_size,dropout):\n",
    "        super().__init__()\n",
    "        self.num_layer=num_layers\n",
    "        self.decoderLayers = nn.ModuleList([TransformerDecoderLayer(\n",
    "            embed_size,num_attention_heads,intermediate_size,dropout) for _ in range(self.num_layer)])\n",
    "    def forward(self,memory,tgt,mask=None):\n",
    "        output=tgt\n",
    "        for layer in self.decoderLayers:\n",
    "            output=layer(memory,output)\n",
    "        return output   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b1c5c52-b077-4c7b-b8e5-48da7a82793b",
   "metadata": {},
   "source": [
    "<h2>Define class for training the model</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28f68d7c-9ebc-4e93-a0b0-59f241652a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self,\n",
    "                 batch_size,# based on our experience for codons average length 300-500, it's beeter to be 10% of the number of samples (base two)\n",
    "                 epochs,\n",
    "                 embed_size,\n",
    "                 path_amino=path_amino,\n",
    "                 path_dna=path_dna,\n",
    "                 checkpoint_dir=checkpoint_dir, #the best model parameters location to save\n",
    "                 path_save_prediction=path_save_prediction,\n",
    "                 tokenizer_DNAfile=tokenizer_DNAfile, #pre_trained tokenized file location\n",
    "                 tokenizer_RNAfile=tokenizer_RNAfile,\n",
    "                 tokenizer_AAfile=tokenizer_AAfile,\n",
    "                 early_stopping_steps=None, #After N steps without improvement, the algorithm will stop.\n",
    "                 val_size=0.1,\n",
    "                 test_size=0.1,\n",
    "                 min_seq_length=0,\n",
    "                 max_seq_length=float('inf'),\n",
    "                 num_seq=None,\n",
    "                 num_attention_heads=2,\n",
    "                 num_layers=3,\n",
    "                 dropout=0.2,\n",
    "                 model_name='best_model.pth'):\n",
    "\n",
    "        super().__init__()\n",
    "        self.src_vocab_size=26\n",
    "        self.tgt_vocab_size=69\n",
    "        self.batch_size=batch_size\n",
    "        self.epochs=epochs\n",
    "        self.embed_size=embed_size\n",
    "        self.intermediate_size=4*self.embed_size\n",
    "        self.early_stopping_steps=early_stopping_steps\n",
    "        self.val_size=val_size\n",
    "        self.test_size=test_size\n",
    "        self.min_seq_length=min_seq_length\n",
    "        self.max_seq_length=max_seq_length\n",
    "        self.num_seq=num_seq\n",
    "        self.num_attention_heads=num_attention_heads\n",
    "        self.num_layers=num_layers\n",
    "        self.dropout=dropout\n",
    "        self.model_name=model_name\n",
    "        self.path_amino=path_amino\n",
    "        self.path_dna=path_dna\n",
    "        self.tokenizer_DNAfile=tokenizer_DNAfile\n",
    "        self.tokenizer_RNAfile=tokenizer_RNAfile\n",
    "        self.tokenizer_AAfile=tokenizer_AAfile\n",
    "        self.checkpoint_dir=checkpoint_dir\n",
    "        self.path_save_prediction=path_save_prediction\n",
    "        self.device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        # Create data splits\n",
    "        print('Processing the data...')\n",
    "        (self.splits, self.num_of_samples, self.input_length, self.output_length,min_raw_seq_length,max_raw_seq_length,min_seq_length,\n",
    "         max_seq_length,min_gc,max_gc)=self.__create_dataset(self.path_amino, self.path_dna, self.test_size, self.val_size,self.min_seq_length,\n",
    "                                                             self.max_seq_length, self.num_seq)\n",
    "        # Create datasets for each split\n",
    "        self.train_dataset = CustomDataset(self.splits['train'],self.tokenizer_DNAfile,self.tokenizer_RNAfile,self.tokenizer_AAfile,\n",
    "                                           self.input_length,self.output_length)\n",
    "        self.val_dataset = CustomDataset(self.splits['valid'],self.tokenizer_DNAfile,self.tokenizer_RNAfile,self.tokenizer_AAfile,\n",
    "                                         self.input_length,self.output_length)\n",
    "        self.test_dataset = CustomDataset(self.splits['test'],self.tokenizer_DNAfile,self.tokenizer_RNAfile,self.tokenizer_AAfile,\n",
    "                                          self.input_length,self.output_length)\n",
    "        self.num_of_train_samples=len(self.train_dataset),\n",
    "        self.num_of_valid_samples=len(self.val_dataset),\n",
    "        self.num_of_test_samples=len(self.test_dataset),\n",
    "        print('The data was divided into training, validation, and test sets:')\n",
    "        print(f'number of the taraining dataset:  {self.num_of_train_samples[0]}')\n",
    "        print(f'number of the validation dataset: {self.num_of_valid_samples[0]}')\n",
    "        print(f'number of the test dataset:       {self.num_of_test_samples[0]}')\n",
    "        print(f'min sequence length of the data:   {min_seq_length} (before filtering: {min_raw_seq_length})')\n",
    "        print(f'max sequence length of the data:   {max_seq_length} (before filtering: {max_raw_seq_length})')\n",
    "        print(f'min GC content of the sequences:   {min_gc}%')\n",
    "        print(f'max GC content of the sequences:   {max_gc}%\\n')\n",
    "        # Create dataloaders\n",
    "        self.train_dataloader = DataLoader(self.train_dataset, batch_size=self.batch_size, shuffle=True)\n",
    "        self.val_dataloader = DataLoader(self.val_dataset, batch_size=self.batch_size, shuffle=False)\n",
    "        self.test_dataloader = DataLoader(self.test_dataset, batch_size=self.batch_size, shuffle=False)\n",
    "        self.src_embedding = Embeddings(self.input_length,self.embed_size,self.src_vocab_size,self.dropout,'aa')\n",
    "        self.tgt_embedding = Embeddings(self.output_length,self.embed_size,self.tgt_vocab_size,self.dropout,'dna')\n",
    "        self.encoder = TransformerEncoder(self.num_layers,self.embed_size,self.num_attention_heads,self.intermediate_size,self.dropout)\n",
    "        self.decoder = TransformerDecoder(self.num_layers,self.embed_size,self.num_attention_heads,self.intermediate_size,self.dropout)\n",
    "        self.fc_out = nn.Linear(self.embed_size,self.tgt_vocab_size)\n",
    "        self.best_model_path = os.path.join(self.checkpoint_dir, self.model_name)\n",
    "        self.passed_epoch = 0\n",
    "        \n",
    "    def forward(self, src, tgt):\n",
    "        src_embed = self.src_embedding(src)\n",
    "        tgt_embed = self.tgt_embedding(tgt)\n",
    "        memory = self.encoder(src_embed)        \n",
    "        output = self.decoder(memory,tgt_embed)\n",
    "        # logits\n",
    "        return self.fc_out(output)\n",
    "    def __create_dataset(self,path_amino, path_dna, test_size, val_size, min_seq_length, max_seq_length, num_seq):\n",
    "        \"\"\"\n",
    "            To create your dataset and divide it into Train, Valid and Test set\n",
    "            \n",
    "            Args:\n",
    "            - path_amino: The location of the amino acids file.\n",
    "            - path_dna:The location of the codons file.\n",
    "            - test_size:The percentage of your data that should be specified to the test dataset.\n",
    "            - val_size:The percentage of your data (after decreasing the test data set from it) that should be specified to the validation dataset.\n",
    "            - min_seq_length:The minimum length of sequence for dataset (default 0)\n",
    "            - max_seq_length:The maximum length of sequence for dataset (default inf)\n",
    "            - num_seq:The number of sequence (default None, it means all of your data after considering \n",
    "              the min_seq_length and max_seq_length will be considered.)\n",
    "            \n",
    "            Returns:\n",
    "            - dataset,len(aa),input_length,output_length\n",
    "        \"\"\"   \n",
    "        aa = st.fasta_to_list(path_amino, seq_to_codon=False, separate_aa=True, sos_eos=False)\n",
    "        cds = st.fasta_to_list(path_dna, seq_to_codon=True, separate_aa=False, sos_eos=True)\n",
    "        length=[len(a.split(' ')) for a in aa]\n",
    "        min_raw_seq_length=min(length)\n",
    "        max_raw_seq_length=max(length)\n",
    "        index_min = [i for i, v in enumerate(aa) if len(v.split(' ')) <= min_seq_length]\n",
    "        index_max = [i for i, v in enumerate(aa) if len(v.split(' ')) >= max_seq_length]\n",
    "        indices_to_remove = list(set(index_min + index_max))\n",
    "        aa=np.delete(aa,indices_to_remove)\n",
    "        cds=np.delete(cds,indices_to_remove)\n",
    "        length=[len(a.split(' ')) for a in aa]\n",
    "        gc_content=[self.__gc_content(c) for c in cds]\n",
    "        min_seq_length=min(length)\n",
    "        max_seq_length=max(length)\n",
    "        min_gc=min(gc_content)\n",
    "        max_gc=max(gc_content)\n",
    "        if num_seq!=None:\n",
    "            aa=aa[:num_seq]\n",
    "            cds=cds[:num_seq]\n",
    "        input_length = max(len(seq.split()) for seq in aa)\n",
    "        output_length = max(len(seq.split()) for seq in cds)\n",
    "        raw_data = {\"input\": aa, 'target': cds}\n",
    "        ds = HFdataset.from_dict(raw_data)\n",
    "        ds = ds.train_test_split(test_size=test_size)\n",
    "        valid = ds['train'].train_test_split(test_size=val_size)\n",
    "        ds['train'] = valid['train']\n",
    "        ds['valid'] = valid['test']\n",
    "        return ds,len(aa), input_length, output_length,min_raw_seq_length,max_raw_seq_length,min_seq_length,max_seq_length,min_gc,max_gc\n",
    "\n",
    "    def train_model(self,optimizer,criterion):\n",
    "        num_epochs = self.epochs\n",
    "        best_val_loss = float('inf')\n",
    "        os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "        train_loss_values=[]\n",
    "        valid_loss_values=[]\n",
    "        train_acc_values=[]\n",
    "        valid_acc_values=[]\n",
    "        early_stopping=0\n",
    "        lr=[]\n",
    "        print('Training is beginning...')\n",
    "        # Training loop\n",
    "        for epoch in range(num_epochs):\n",
    "            self.train()\n",
    "            total_train_loss = 0\n",
    "            total_train_correct=0\n",
    "            total_train_masked_length=0\n",
    "            self.passed_epoch+=1\n",
    "            for src, tgt in self.train_dataloader:  #for each batch\n",
    "                src, tgt = src.to(self.device), tgt.to(self.device)\n",
    "                tgt_input = tgt[:, :-1]\n",
    "                tgt_output = tgt[:, 1:]\n",
    "                optimizer.zero_grad()\n",
    "                output = self(src, tgt_input)\n",
    "                loss = criterion(output.view(-1, output.size(-1)), tgt_output.contiguous().view(-1))\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                total_train_loss += loss.item()\n",
    "                batch_accuracy,correct,masked_length = self.masked_accuracy(tgt_output,output,pad_token_id=0)\n",
    "                total_train_correct+=correct\n",
    "                total_train_masked_length+=masked_length \n",
    "            accuracy_train=total_train_correct / total_train_masked_length  \n",
    "            avg_train_loss = total_train_loss / len(self.train_dataloader)\n",
    "            # Validation step\n",
    "            avg_val_loss, accuracy_valid =self.test(self.val_dataloader, criterion)\n",
    "            scheduler.step(avg_val_loss)\n",
    "            #append data to plot.\n",
    "            train_loss_values.append(avg_train_loss)\n",
    "            valid_loss_values.append(avg_val_loss)\n",
    "            train_acc_values.append(accuracy_train)\n",
    "            valid_acc_values.append(accuracy_valid)\n",
    "            if scheduler.get_last_lr()[0]<optimizer.param_groups[0]['lr']:\n",
    "                print(f'The learning rate changed to {scheduler.get_last_lr}')\n",
    "            lr.append(scheduler.get_last_lr()[0])\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], '\n",
    "                  f'Loss: {avg_train_loss:.4f}, '\n",
    "                  f'Train Accuracy: {accuracy_train*100:.2f}%, '\n",
    "                  f'Validation Loss: {avg_val_loss:.4f}, '\n",
    "                  f'Validation Accuracy: {accuracy_valid*100:.2f}%')        \n",
    "            # Save the best model\n",
    "            if avg_val_loss < best_val_loss:\n",
    "                best_val_loss = avg_val_loss \n",
    "                self.save_checkpoint(epoch+1, optimizer, avg_val_loss, self.best_model_path)\n",
    "                early_stopping=0\n",
    "                schedule_lr=0\n",
    "                print(f'The Best model saved at epoch {epoch+1}')\n",
    "            else:\n",
    "                early_stopping+=1\n",
    "\n",
    "            if early_stopping==self.early_stopping_steps:\n",
    "                print(f'***Early stopping in epoch {epoch+1}***')\n",
    "                break                       \n",
    "        return train_loss_values, valid_loss_values, train_acc_values, valid_acc_values, lr\n",
    "    \n",
    "    # Evaluation function\n",
    "    def test(self, test_dataloader, criterion):\n",
    "        self.eval()\n",
    "        total_test_loss = 0\n",
    "        total_test_correct = 0\n",
    "        total_test_masked_length = 0\n",
    "        with torch.no_grad():\n",
    "            for src, tgt in test_dataloader:  # for each test batch\n",
    "                src, tgt = src.to(self.device), tgt.to(self.device)\n",
    "                tgt_input = tgt[:, :-1]\n",
    "                tgt_output = tgt[:, 1:]\n",
    "                output = self(src, tgt_input)\n",
    "                loss = criterion(output.view(-1, output.size(-1)), tgt_output.contiguous().view(-1))\n",
    "                total_test_loss += loss.item()\n",
    "                batch_accuracy, correct, masked_length = self.masked_accuracy(tgt_output, output, pad_token_id=0)\n",
    "                total_test_correct += correct\n",
    "                total_test_masked_length += masked_length\n",
    "    \n",
    "        avg_test_loss = total_test_loss / len(test_dataloader)\n",
    "        accuracy_test = total_test_correct / total_test_masked_length\n",
    "        return avg_test_loss, accuracy_test\n",
    "        print(f'Test Loss: {avg_test_loss:.4f}, Test Accuracy: {accuracy_test:.4f}')\n",
    "\n",
    "    # Define the inference function\n",
    "    def infer(self, path_infer,start_token_id=2,output_leval='DNA',codons_percentage=True,codon_biased=None,sample_nucloeus=False,nucleus_threshold=0.9):      \n",
    "        \"\"\"\n",
    "        Predict the codon related to the specified protein.\n",
    "    \n",
    "        Args:\n",
    "        - path_infer: The address of your protein FASTA format.\n",
    "        - start_token_id: it is the 'sos_token' (e.g., '[SOS]') number in the Tokenizing process section (Default is 2).\n",
    "        - output_leval: type of codons that the algorithm has beed trained based on it, (e.g., 'DNA' or 'RNA').\n",
    "        - codons_percentage: if True, the percentage of each codons related to an amino acid will be printed.\n",
    "        - codon_biased: a dictionary type of the organism codon biased for further use (ex.  tobacco_biased={'F':'UUU','L':'CUU','I':'AUU',...}\n",
    "        - sample_nucleous: if True, change the stratefy of selecting next token based on the nucleous sampling method.\n",
    "        - nucleus_threshold: if sample_nucleous True, a threshold should be assigned, default=0.9\n",
    "    \n",
    "        Returns:\n",
    "        - predicted codon and some other useful information related to the predicted codon, like GC content.\n",
    "        \"\"\"\n",
    "        if output_leval=='DNA':\n",
    "            tokenizer = Tokenizer.from_file(tokenizer_DNAfile)\n",
    "        else:\n",
    "            tokenizer = Tokenizer.from_file(tokenizer_RNAfile)\n",
    "        self.eval()\n",
    "        infer_data = st.fasta_to_list(path_infer, seq_to_codon=False, separate_aa=True, sos_eos=False)\n",
    "        fast_tokenizer = PreTrainedTokenizerFast(tokenizer_file=tokenizer_AAfile)\n",
    "        src=fast_tokenizer.encode(infer_data[0],padding='max_length', return_tensors=\"pt\",max_length=self.input_length)\n",
    "        max_length=torch.count_nonzero(src).item()\n",
    "        with torch.no_grad():\n",
    "            src_embed = self.src_embedding(src)\n",
    "            memory = self.encoder(src_embed)\n",
    "            tgt_input = torch.LongTensor([[start_token_id]])\n",
    "            output_sequence = [start_token_id]\n",
    "            for _ in range(max_length):\n",
    "                tgt_embed = self.tgt_embedding(tgt_input)\n",
    "                decoder_output = self.decoder(memory, tgt_embed)\n",
    "                output_logits = self.fc_out(decoder_output[:, -1, :])\n",
    "                next_token = output_logits[0].argmax(dim=-1).item() # Greedy algorithm\n",
    "                if sample_nucloeus: # Nucleous sampling algorithm\n",
    "                    next_token=nucleous_sampling(output_logits,threshold)            \n",
    "                output_sequence.append(next_token)\n",
    "                tgt_input = torch.cat((tgt_input, torch.LongTensor([[next_token]])), dim=1)\n",
    "        codon=tokenizer.decode(output_sequence[1:], skip_special_tokens=False).upper()\n",
    "        # Create the content in FASTA format\n",
    "        fasta_content = f\">Predicted Sequence\\n{codon}\\n\"\n",
    "        # Write the string to the text file\n",
    "        with open(path_save_prediction, 'w') as file:\n",
    "            file.write(fasta_content)\n",
    "        gc_content=self.__gc_content(codon,sep=' ')\n",
    "        wrong_codon_num,index_num=self.__mapcodon(infer_data[0],codon)\n",
    "        print(f'PREDICTED Codon:\\n {codon}\\n')        \n",
    "        print(f\"The predicted codon sequence has been saved in {path_save_prediction}\")\n",
    "        print('-'*50)\n",
    "        print(f'GC content: {gc_content}%')\n",
    "        print('-'*50)\n",
    "        print(f'The number of codons that mapped wrongly to an amino acid: {wrong_codon_num} of {len(codon.split(' '))}')\n",
    "        print(f'accuracy percentage of mapped codons: {round((1-wrong_codon_num/len(codon.split(' ')))*100,2)}%')\n",
    "        print('-'*50)\n",
    "        if codons_percentage:\n",
    "            self.__codons_percentage(infer_data[0],codon,codon_biased)\n",
    "        return codon\n",
    "\n",
    "    def masked_accuracy(self,label, pred, pad_token_id=0):\n",
    "        \"\"\"\n",
    "        To calculate the masked accuracy.\n",
    "    \n",
    "        Args:\n",
    "        - label (torch.Tensor): Ground truth labels with padding.\n",
    "        - pred (torch.Tensor): Predictions from the model (logits).\n",
    "        - pad_token_id (int): The id used for padding.\n",
    "    \n",
    "        Returns:\n",
    "        - The masked accuracy value.\n",
    "        - number of matches\n",
    "        - numer of labels which is not equal to zero.\n",
    "        \"\"\"\n",
    "        pred = pred.argmax(dim=-1)\n",
    "        mask = label != pad_token_id\n",
    "        match = (label == pred) & mask\n",
    "        match = match.float()\n",
    "        mask = mask.float()\n",
    "        accuracy = match.sum() / mask.sum()\n",
    "        return accuracy.item(),match.sum().item(),mask.sum().item()\n",
    "        \n",
    "    def __codons_percentage(self,aa, cds, codon_biased=None,sep=' '):\n",
    "        \"\"\"To calculate the percentage of each amino acid codon selected by algorithm. if you define a codon biased dictionary, the codon-biased\n",
    "        related to an amono acid will be marked.\"\"\"\n",
    "        cds = cds.split(sep)\n",
    "        aa = aa.split(sep)\n",
    "        dict_aa = {}\n",
    "        dict_bb = {}\n",
    "        percentages = {}\n",
    "        # Convert codon biased in DNA format\n",
    "        if codon_biased is not None:\n",
    "            for i in codon_biased.values():\n",
    "                if 'U' in i:\n",
    "                    codon_biased = {key: value.replace('U', 'T') for key, value in tobacco_biased.items()}\n",
    "                    break\n",
    "        for i, v in enumerate(list(aa)):\n",
    "            if v in dict_aa:\n",
    "                dict_aa[v].append(cds[i])\n",
    "            else:\n",
    "                dict_aa[v] = [cds[i]]\n",
    "            if (codon_biased is not None) and cds[i] == codon_biased.get(v, None):\n",
    "                dict_aa[v][-1] = '*' + cds[i]\n",
    "        for k, v in dict_aa.items():\n",
    "            dict_bb[k] = Counter(v)    \n",
    "        # Calculate the percentage of each codon relative to its amino acid\n",
    "        for amino_acid, codons in dict_bb.items():\n",
    "            total_count = sum(codons.values())\n",
    "            percentages[amino_acid] = {codon: (count / total_count) * 100 for codon, count in codons.items()}      \n",
    "        print('Percentage of selected codons for each amino acid:')              \n",
    "        print('*: biased codon', end='\\n\\n')       \n",
    "        for amino_acid, codons in percentages.items():\n",
    "            print(f\"{amino_acid}:\")\n",
    "            for codon, percent in codons.items():\n",
    "                print(f\"  {codon}: {percent:.2f}%\")\n",
    "\n",
    "    def __gc_content(self,seq,sep=' '):\n",
    "        '''To calculate the GC content in a DNA/RNA sequence'''\n",
    "        seq=seq.replace(sep,\"\")\n",
    "        return round((seq.count('C')+seq.count('G'))/len(seq)*100)\n",
    "        \n",
    "    def __mapcodon(self,amino,codon,sep=' '):\n",
    "        \"\"\"To calculate if each predicted codon is the right codon for the specified amino acid\"\"\"\n",
    "        count=0\n",
    "        index=[]\n",
    "        amino=amino.split(sep)\n",
    "        codon=codon.split(sep)\n",
    "        special_tokens=['[PAD]','[UNK]','[SOS]','[EOS]','[MASK]']\n",
    "        for i,v in enumerate(amino):\n",
    "            if (codon[i] not in special_tokens) and (v!=st.DNA_Codons[codon[i]]):\n",
    "                index.append(i)\n",
    "                count+=1 \n",
    "        return count, index\n",
    "\n",
    "    def nucleous_sampling(self,logits,threshold):\n",
    "        \"\"\"choose the next token based on the method written in this paper: https://arxiv.org/pdf/1904.09751\n",
    "        unlike previous methos like greedy search (argmax), top_k or beam search, this method chose the best next token among the smallest list\n",
    "        that the probability cumulative of their candidates is equal or greater than the specified threshold. \"\"\"\n",
    "        cumulative_sum = 0.0\n",
    "        nucleous_list=[]\n",
    "        softmax=torch.softmax(logits,dim=-1)[0]\n",
    "        # tensor_list=torch.softmax(softmax,dim=-1)\n",
    "        sorted_tensor = sorted(enumerate(softmax), key=lambda x: x[1],reverse=True)\n",
    "        # Find the index where the cumulative sum exceeds threshold\n",
    "        for index, (original_index, value) in enumerate(sorted_tensor):\n",
    "            nucleous_list.append((original_index, value))\n",
    "            cumulative_sum += value.item()\n",
    "            if cumulative_sum > threshold:\n",
    "                break\n",
    "        rnd=np.random.randint(0,len(nucleous_list))\n",
    "        return nucleous_list[rnd][0]\n",
    "        \n",
    "    def save_checkpoint(self,epoch, optimizer, loss, path):\n",
    "        state = {\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': self.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': loss}\n",
    "        torch.save(state, path)\n",
    "        \n",
    "    def load_checkpoint(self,path,optimizer):\n",
    "        checkpoint = torch.load(path)\n",
    "        self.load_state_dict(checkpoint['model_state_dict'])\n",
    "        if optimizer:\n",
    "            optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        epoch = checkpoint['epoch']\n",
    "        loss = checkpoint['loss']\n",
    "        return epoch, loss\n",
    "\n",
    "    #Error analysis function\n",
    "    def error_analysis(self, data=None, output_level='DNA',norm_cm=None):\n",
    "        \"\"\"\n",
    "        Perform error analysis on the data.\n",
    "    \n",
    "        Args:\n",
    "        - data: The data that was batched using the CustomDataset class.(if None, the model test dataset (test_dataloader) will be assigned)\n",
    "        - output_level: the type of codons that the algorithm has beed trained based on it, (e.g., 'DNA' or 'RNA') default 'DNA'\n",
    "        - norm_cm: normalize : {'true', 'pred', 'all'}, default=None\n",
    "          Normalizes confusion matrix over the true (rows), predicted (columns) conditions or all the population.\n",
    "          If None, confusion matrix will not be normalized.\n",
    "    \n",
    "        Returns:\n",
    "        - Sample-accuracy for each quartile (totally four accuracy values for each sequence)\n",
    "        - Substitution-accuracy that is the accuracy achieved when both relevant codons in the label and prediction correspond to similar amino acids.\n",
    "          (ex. prediciton codon:\"AAT\", label codon:\"AAC\", but because both codons are related to amino acid \"N\", it is considered True for calculating\n",
    "          the accuracy metric value. in the sample accuracy or accuracy value during training process it is considered False)\n",
    "        - Substitution_accuracy_mean that is the total average of the Substitution-accuracy value.\n",
    "        - cf_matrix that is the confusion matrix\n",
    "        \n",
    "        * Substitution-accuracy and Substitution_accuracy_mean are presented as one tuple (e.g., (Substitution-accuracy, Substitution_accuracy_mean))\n",
    "          example:\n",
    "          sample_acc,substitution,cf_matrix = model.error_analysis()\n",
    "          substitution[0] => Substitution-accuracy\n",
    "          substitution[1] => Substitution_accuracy_mean\n",
    "\n",
    "        * at the end of the process analysis a heatmap diagram for average accuracy value of each quartile, the confusion matrix table and\n",
    "          the precision,recall and F1_score table for each amino acid will be plotted.\n",
    "          \n",
    "        \"\"\"     \n",
    "        self.eval()\n",
    "        samples_accuracy=[]\n",
    "        substitution_accuracy=[]\n",
    "        confusion_dict={}\n",
    "        y_true=[]\n",
    "        y_pred=[]\n",
    "        if data is None:\n",
    "            data=self.test_dataloader\n",
    "        if output_level=='DNA':\n",
    "            tokenizer = Tokenizer.from_file(tokenizer_DNAfile)\n",
    "        else:\n",
    "            tokenizer = Tokenizer.from_file(tokenizer_RNAfile)\n",
    "        with torch.no_grad():\n",
    "            for src, tgt in data:  # for each test batch\n",
    "                src, tgt = src.to(self.device), tgt.to(self.device)\n",
    "                tgt_input = tgt[:, :-1]\n",
    "                tgt_output = tgt[:, 1:]\n",
    "                output = self(src, tgt_input)\n",
    "                #calculate quartile accuracies\n",
    "                for target,pred in zip(tgt_output,output): \n",
    "                    #because each sample has different pad_token length so for calculating the quartiles we have to analyze each sample one bat a time\n",
    "                    true_indices = torch.where(target != 0)[0]\n",
    "                    length = len(true_indices)\n",
    "                    pred=pred.argmax(-1)\n",
    "                    y_true.append(target)\n",
    "                    y_pred.append(pred)\n",
    "                    if length == 0:\n",
    "                        continue  # skip if there are no true values\n",
    "                    quartile_indices = [int(length * p) for p in [0.25, 0.50, 0.75, 1.00]]\n",
    "                    accuracies = []\n",
    "                    start_idx = 0\n",
    "                    for end_idx in quartile_indices:\n",
    "                        pred_quartile = pred[start_idx:end_idx]\n",
    "                        target_quartile = target[start_idx:end_idx]\n",
    "                        match_quartile = (pred_quartile == target_quartile)\n",
    "                        accuracy = torch.sum(match_quartile).float() / len(match_quartile)\n",
    "                        accuracies.append(accuracy.item())\n",
    "                        start_idx = end_idx\n",
    "                    samples_accuracy.append(accuracies)\n",
    "                    # calculate substitution accuracy\n",
    "                    false_pred=(pred!=target) & (target!=0)\n",
    "                    mask=(target!=0)\n",
    "                    match = (target == pred) & mask\n",
    "                    substitution_match=0\n",
    "                    for i,is_false in enumerate(false_pred):\n",
    "                        if is_false: # [EOS] token is 3\n",
    "                            codon_pred=tokenizer.decode([pred[i]], skip_special_tokens=True).upper()\n",
    "                            codon_label=tokenizer.decode([target[i]], skip_special_tokens=True).upper()\n",
    "                            if codon_pred!='':\n",
    "                                amino_pred=st.DNA_Codons[codon_pred]\n",
    "                            if codon_label!='':\n",
    "                                amino_label=st.DNA_Codons[codon_label]\n",
    "                            if amino_pred==amino_label:\n",
    "                                substitution_match+=1\n",
    "                    for i in range(torch.argmin(target)):\n",
    "                        if target[i].item() not in confusion_dict.keys():\n",
    "                            confusion_dict[target[i].item()]=[pred[i].item()]\n",
    "                        else:\n",
    "                            confusion_dict[target[i].item()].append(pred[i].item())\n",
    "                        \n",
    "\n",
    "                    substitution_accuracy.append(((match.sum()+substitution_match)/mask.sum()).item())\n",
    "                    substitution_accuracy_mean=torch.tensor(np.mean(substitution_accuracy))\n",
    "        samples_accuracy=torch.tensor(samples_accuracy)\n",
    "        samples_accuracy_mean=torch.mean(samples_accuracy,dim=0).unsqueeze(0)\n",
    "        # Create a heatmap\n",
    "        plt.figure(figsize=(6, 4))\n",
    "        heatmap = sns.heatmap(samples_accuracy_mean, annot=True, cmap=\"YlGnBu\", cbar=True, \n",
    "                              xticklabels=['Quartile 1', 'Quartile 2', 'Quartile 3', 'Quartile 4'])\n",
    "        heatmap.set_yticklabels([]) # Remove y-axis label (0) by setting it to an empty string\n",
    "        plt.title('Average accuracy of Each Quartile')\n",
    "        print(f'plotting the average accuracy of Each Quartile...')\n",
    "        plt.show()  \n",
    "        #plot confusion matrix\n",
    "        y_true=np.ravel(y_true)\n",
    "        y_pred=np.ravel(y_pred)\n",
    "        filt=np.where(y_true!=0)\n",
    "        y_true_filtered=y_true[filt]\n",
    "        y_pred_filtered=y_pred[filt]\n",
    "        cf_matrix = confusion_matrix(y_true_filtered,y_pred_filtered,normalize=norm_cm)\n",
    "        labels = [tokenizer.decode([i],skip_special_tokens=False).upper() for i in np.unique(y_true_filtered)]\n",
    "        ticks=np.linspace(0, len(labels),num=len(labels))\n",
    "        plt.figure(figsize=(15, 12))\n",
    "        plt.imshow(cf_matrix, interpolation='none', cmap='Blues')\n",
    "        plt.colorbar()\n",
    "        plt.xticks(ticks, labels, fontsize=8, rotation=90)\n",
    "        plt.gca().xaxis.set_ticks_position('top')\n",
    "        plt.xlabel('Predicted')\n",
    "        plt.yticks(ticks, labels, fontsize=8)\n",
    "        plt.ylabel('Actual')\n",
    "        if norm_cm=='true':\n",
    "            plt.title('Confusion matrix (Normalized over the true (rows))')\n",
    "        elif norm_cm=='pred':\n",
    "            plt.title('Confusion matrix (Normalized over the predicted (columns))')\n",
    "        elif norm_cm=='all':\n",
    "            plt.title('Confusion matrix (Normalized over the all the population)')\n",
    "        else:\n",
    "            plt.title('Confusion matrix')\n",
    "        print(f'\\nplotting confusion matrix...')\n",
    "        plt.show()\n",
    "        \n",
    "        #plot precisin, recall, F1_score table\n",
    "        prf=precision_recall_fscore_support(y_true_filtered,y_pred_filtered,zero_division=0)\n",
    "        reshaped_prf = [np.round(array.reshape(-1, 1),2) for array in prf]\n",
    "        stacked_arrays = np.hstack([np.array(labels).reshape(-1,1)] + reshaped_prf)\n",
    "        prf_df=pd.DataFrame(stacked_arrays,columns=['Amino acid','Precision','Recall','F1-score','Count'])\n",
    "        prf_df.sort_values(by='Recall',inplace=True,ascending=False)\n",
    "        print(f'\\nplotting precisin, recall, F1_score table (sorted descending by recall)...')\n",
    "        print(f'* Precision means how many percentage of the predicted values for that amino acid was predicted correctly')\n",
    "        print(f'* Recall means how many percentage of the specific amino acid was predicted correctly')\n",
    "        print(f'* F1-score is the harmonic average of the precision and recall values')\n",
    "        print(f\"**The index numbers correspond to the rows of the confusion matrix for the relevant codons; that's why they have not been reset^^\\n\")\n",
    "        display(prf_df)\n",
    "        return samples_accuracy,(torch.tensor(substitution_accuracy),substitution_accuracy_mean),cf_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8317f45-c111-4580-9fe9-0a49b7089c68",
   "metadata": {},
   "source": [
    "<h2>Training and Evaluating the data</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "81fc2f01-1441-4e79-9ac3-27bd732858bd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing the data...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e9bc758db0045eea994f0925076d42e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/162 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87a86d92ef464dd1b26b2fbe3eb8c5fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/18 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e9c3c596fb74ec29b1c9e0ce9bbad86",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/20 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The data was divided into training, validation, and test sets:\n",
      "number of the taraining dataset:  162\n",
      "number of the validation dataset: 18\n",
      "number of the test dataset:       20\n",
      "min sequence length of the data:   101 (before filtering: 23)\n",
      "max sequence length of the data:   199 (before filtering: 5490)\n",
      "min GC content of the sequences:   16%\n",
      "max GC content of the sequences:   81%\n",
      "\n",
      "Training is beginning...\n",
      "Epoch [1/50], Loss: 4.1900, Train Accuracy: 3.05%, Validation Loss: 4.0830, Validation Accuracy: 3.56%\n",
      "The Best model saved at epoch 1\n",
      "Epoch [2/50], Loss: 4.0933, Train Accuracy: 3.60%, Validation Loss: 4.0733, Validation Accuracy: 4.79%\n",
      "The Best model saved at epoch 2\n",
      "Epoch [3/50], Loss: 4.0386, Train Accuracy: 4.39%, Validation Loss: 3.9941, Validation Accuracy: 4.67%\n",
      "The Best model saved at epoch 3\n",
      "Epoch [4/50], Loss: 4.0035, Train Accuracy: 4.39%, Validation Loss: 4.1342, Validation Accuracy: 3.45%\n",
      "Epoch [5/50], Loss: 4.0163, Train Accuracy: 4.57%, Validation Loss: 3.9814, Validation Accuracy: 5.74%\n",
      "The Best model saved at epoch 5\n",
      "Epoch [6/50], Loss: 3.9051, Train Accuracy: 6.21%, Validation Loss: 3.9717, Validation Accuracy: 5.25%\n",
      "The Best model saved at epoch 6\n",
      "Epoch [7/50], Loss: 3.8372, Train Accuracy: 7.09%, Validation Loss: 3.9991, Validation Accuracy: 5.90%\n",
      "Epoch [8/50], Loss: 3.8086, Train Accuracy: 8.54%, Validation Loss: 3.9258, Validation Accuracy: 6.66%\n",
      "The Best model saved at epoch 8\n",
      "Epoch [9/50], Loss: 3.7314, Train Accuracy: 9.61%, Validation Loss: 3.9487, Validation Accuracy: 7.04%\n",
      "Epoch [10/50], Loss: 3.6898, Train Accuracy: 10.43%, Validation Loss: 3.9754, Validation Accuracy: 6.36%\n",
      "Epoch [11/50], Loss: 3.6697, Train Accuracy: 10.95%, Validation Loss: 3.9068, Validation Accuracy: 6.89%\n",
      "The Best model saved at epoch 11\n",
      "Epoch [12/50], Loss: 3.5800, Train Accuracy: 11.93%, Validation Loss: 3.9383, Validation Accuracy: 7.35%\n",
      "Epoch [13/50], Loss: 3.5772, Train Accuracy: 12.77%, Validation Loss: 3.9019, Validation Accuracy: 6.62%\n",
      "The Best model saved at epoch 13\n",
      "Epoch [14/50], Loss: 3.5217, Train Accuracy: 13.94%, Validation Loss: 3.8916, Validation Accuracy: 6.16%\n",
      "The Best model saved at epoch 14\n",
      "Epoch [15/50], Loss: 3.4520, Train Accuracy: 15.43%, Validation Loss: 3.8906, Validation Accuracy: 7.50%\n",
      "The Best model saved at epoch 15\n",
      "Epoch [16/50], Loss: 3.4195, Train Accuracy: 16.49%, Validation Loss: 3.8604, Validation Accuracy: 7.58%\n",
      "The Best model saved at epoch 16\n",
      "Epoch [17/50], Loss: 3.3057, Train Accuracy: 17.99%, Validation Loss: 3.8521, Validation Accuracy: 7.92%\n",
      "The Best model saved at epoch 17\n",
      "Epoch [18/50], Loss: 3.2338, Train Accuracy: 19.34%, Validation Loss: 3.8594, Validation Accuracy: 8.38%\n",
      "Epoch [19/50], Loss: 3.2154, Train Accuracy: 20.14%, Validation Loss: 3.8360, Validation Accuracy: 9.46%\n",
      "The Best model saved at epoch 19\n",
      "Epoch [20/50], Loss: 3.0171, Train Accuracy: 21.94%, Validation Loss: 3.8305, Validation Accuracy: 9.30%\n",
      "The Best model saved at epoch 20\n",
      "Epoch [21/50], Loss: 3.0652, Train Accuracy: 23.87%, Validation Loss: 3.7951, Validation Accuracy: 9.95%\n",
      "The Best model saved at epoch 21\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[31], line 14\u001b[0m\n\u001b[0;32m     11\u001b[0m criterion \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss(ignore_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)  \u001b[38;5;66;03m# Assuming pad_token_id is defined\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m#TRAIN AND EVALUATE THE DATA\u001b[39;00m\n\u001b[1;32m---> 14\u001b[0m train_loss_values,valid_loss_values,train_acc_values,valid_acc_values,lr\u001b[38;5;241m=\u001b[39m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m#PLOT CONCLUSIONS\u001b[39;00m\n\u001b[0;32m     16\u001b[0m x\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39marange(model\u001b[38;5;241m.\u001b[39mpassed_epoch)\n",
      "Cell \u001b[1;32mIn[29], line 160\u001b[0m, in \u001b[0;36mTransformer.train_model\u001b[1;34m(self, optimizer, criterion)\u001b[0m\n\u001b[0;32m    158\u001b[0m tgt_output \u001b[38;5;241m=\u001b[39m tgt[:, \u001b[38;5;241m1\u001b[39m:]\n\u001b[0;32m    159\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m--> 160\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtgt_input\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    161\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(output\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, output\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)), tgt_output\u001b[38;5;241m.\u001b[39mcontiguous()\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m    162\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[29], line 88\u001b[0m, in \u001b[0;36mTransformer.forward\u001b[1;34m(self, src, tgt)\u001b[0m\n\u001b[0;32m     86\u001b[0m tgt_embed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtgt_embedding(tgt)\n\u001b[0;32m     87\u001b[0m memory \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder(src_embed)        \n\u001b[1;32m---> 88\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmemory\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtgt_embed\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     89\u001b[0m \u001b[38;5;66;03m# logits\u001b[39;00m\n\u001b[0;32m     90\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc_out(output)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[28], line 52\u001b[0m, in \u001b[0;36mTransformerDecoder.forward\u001b[1;34m(self, memory, tgt, mask)\u001b[0m\n\u001b[0;32m     50\u001b[0m output\u001b[38;5;241m=\u001b[39mtgt\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoderLayers:\n\u001b[1;32m---> 52\u001b[0m     output\u001b[38;5;241m=\u001b[39m\u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmemory\u001b[49m\u001b[43m,\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[28], line 35\u001b[0m, in \u001b[0;36mTransformerDecoderLayer.forward\u001b[1;34m(self, encoder_output, outputs)\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m,encoder_output,outputs):\n\u001b[1;32m---> 35\u001b[0m     y2 \u001b[38;5;241m=\u001b[39m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdecoder_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43mmask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     36\u001b[0m     y3\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maddnorm(outputs,y2)\n\u001b[0;32m     37\u001b[0m     y4 \u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder_decoder_attention(hidden_state\u001b[38;5;241m=\u001b[39mencoder_output,decoder_state\u001b[38;5;241m=\u001b[39my3,mask\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[27], line 69\u001b[0m, in \u001b[0;36mMultiHeadAttention.forward\u001b[1;34m(self, hidden_state, decoder_state, mask)\u001b[0m\n\u001b[0;32m     68\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, hidden_state,decoder_state,mask):\n\u001b[1;32m---> 69\u001b[0m     x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([\u001b[43mh\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdecoder_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43mmask\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m h \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mheads], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     70\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[27], line 55\u001b[0m, in \u001b[0;36mAttentionHead.forward\u001b[1;34m(self, hidden_state, decoder_state, mask)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m,hidden_state,decoder_state,mask):\n\u001b[0;32m     53\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m decoder_state\u001b[38;5;241m==\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m         \u001b[38;5;66;03m#To calculate self_attention for encoder and also masked self_attention for decoder layer (by passing mask=True).\u001b[39;00m\n\u001b[1;32m---> 55\u001b[0m         attn_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mscaled_dot_product_attention\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mq\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_state\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mk\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_state\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_state\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43mmask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     56\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     57\u001b[0m         \u001b[38;5;66;03m#To calculate encoder-decoder attention in decoder layer\u001b[39;00m\n\u001b[0;32m     58\u001b[0m         attn_outputs \u001b[38;5;241m=\u001b[39m scaled_dot_product_attention(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mq(decoder_state), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mk(hidden_state), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mv(hidden_state),mask\u001b[38;5;241m=\u001b[39mmask)\n",
      "Cell \u001b[1;32mIn[27], line 41\u001b[0m, in \u001b[0;36mscaled_dot_product_attention\u001b[1;34m(query, key, value, mask)\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     40\u001b[0m     mask_matrice\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mtril(torch\u001b[38;5;241m.\u001b[39mones(key\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m1\u001b[39m),key\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m1\u001b[39m)))\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m---> 41\u001b[0m     scores\u001b[38;5;241m=\u001b[39m\u001b[43mscores\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmasked_fill\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmask_matrice\u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;28;43mfloat\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     42\u001b[0m weights\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39msoftmax(scores,dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     43\u001b[0m att_output\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mbmm(weights,value)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = Transformer(batch_size=16,\n",
    "                    epochs=50,\n",
    "                    embed_size=256,\n",
    "                    early_stopping_steps=20,\n",
    "                    min_seq_length=100,\n",
    "                    max_seq_length=500,\n",
    "                    num_seq=200)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "scheduler=torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer,mode='min',factor=.1,patience=100,cooldown=10)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0)  # Assuming pad_token_id is defined\n",
    "\n",
    "#TRAIN AND EVALUATE THE DATA\n",
    "train_loss_values,valid_loss_values,train_acc_values,valid_acc_values,lr=model.train_model(optimizer,criterion)\n",
    "#PLOT CONCLUSIONS\n",
    "x=np.arange(model.passed_epoch)\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 3))  # Adjust the figure size as needed\n",
    "#plot loss function values\n",
    "ax1.plot(x, train_loss_values, label='Train loss')\n",
    "ax1.plot(x, valid_loss_values, label='Validation loss',color='g')\n",
    "ax1.set_xlabel('epochs')\n",
    "ax1.set_ylabel('Values')\n",
    "ax1.set_title('The loss values for train and validation set')\n",
    "ax1.legend(loc='upper right')\n",
    "ax1.grid(True)\n",
    "\n",
    "#plot accuracy values\n",
    "ax2.plot(x, train_acc_values, label='Train accuracy')\n",
    "ax2.plot(x, valid_acc_values, label='Validation accuracy', color='g')\n",
    "ax2.set_xlabel('epochs')\n",
    "ax2.set_ylabel('Values')\n",
    "ax2.set_title('The accuracy values for train and validation set')\n",
    "ax2.legend(loc='upper left')\n",
    "ax2.grid(True)\n",
    "\n",
    "# Adjust layout to prevent overlap\n",
    "plt.subplots_adjust(wspace=0.5)\n",
    "plt.show()\n",
    "\n",
    "# Plot lr on a separate figure\n",
    "plt.figure(figsize=(5, 3))  # Adjust the figure size as needed\n",
    "plt.plot(x, lr, label='lr', color='r')\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('LR')\n",
    "plt.title('Plot of Learning Rate')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52ca8d2b-8f65-432d-b2c5-1c642f8a2d69",
   "metadata": {},
   "source": [
    "<h2>Loading the best model for the test stage</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fa0573b-7698-42b5-88c1-e4fc354ea5f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "loading_path=os.path.join(checkpoint_dir, model.model_name)\n",
    "model.load_checkpoint(loading_path,optimizer)\n",
    "avg_test_loss, accuracy_test=model.test(model.test_dataloader, criterion)\n",
    "print(f'average test data loss: {avg_test_loss:.4f}\\naccuracy of test dataset: {accuracy_test*100:.4f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "985b799a-3ef2-467f-9c8d-3b1ddf59c0ac",
   "metadata": {},
   "source": [
    "<h2>Error analysis</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7da2a36-176e-4587-acb4-2079195d5779",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_acc,substitution=model.error_analysis()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c7f73d5-0262-4bee-822f-58280b726ddd",
   "metadata": {},
   "source": [
    "<h2>The inference stage</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f96c26c-a3cc-4646-ba8e-2d2af5b80da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "z=model.infer(path_infer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "976e7c92-809f-4aaf-90b3-e4391d26c1ce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
